#author Yunfeng Wang 06/24/2025
# research yml that contains the information from https://efosler.github.io/#research


introduction:
  heading: "Introduction"
  paragraphs:
    - text: "This page gives an overview of and links to recent research papers that describe some of the research of my lab. The commentary for some papers gives links to follow-on work so that the reader can see the trajectories of the different research lines."
    - text: "My group's current research covers a number of topics in speech and natural language processing. The overall goal of my lab's research is to find meaningful ways to integrate acoustic, phonetic, lexical, and other linguistic insights into the speech recognition process through a combination of statistical modeling and data/error analysis. My goal is to train students to be flexible, independent thinkers who can apply statistical techniques to a range of language-related problems."
  lab:
      name: my lab
      lab_url: "https://cse.osu.edu/directory"

# data/research.yml

home:
  research:
    title: "Research"
    intro: "My students and I work in a number of areas in speech and language processing, including..."
    topics:
      - "Novel statistical methods for speech recognition"
      - "Prediction of errors in ASR systems"
      - "Discriminative language/pronunciation models"
      - "Phonetically-aware speech enhancement"
      - "Statistical investigations of linguistic phenomena in large corpora"
      - "Spoken dialogue system design; spoken human-computer interface issues"
      - "Natural language generation for spoken dialogue systems"
      - "Information extraction from electronic medical records"
    links:
      - text: "More about lab"
        url: "https://cse.osu.edu/directory"
      - text: "More about Research"
        url: "/research.html"
      - text: "Joining the lab"
        url: "/research.html#osu-note"



joining_lab:
  heading: "Joining the lab"
  description: "The Speech and Language Technologies Laboratory is a group of dynamic researchers who are interested in mixing aspects of machine learning with speech and language processing."
  notes:
    - text: "If you are not an OSU student, but want to apply: see my note on the application process to OSU."
    - text: "If you are a current OSU student: see the once you are at OSU section of my note."
      notes_url: "#osu-note"
  lab:
    name: "Speech and Language Technologies Laboratory"
    lab_url: "https://cse.osu.edu/directory"

osu_note:
  sections:
    - heading: "If you are applying to Ohio State"
      paragraphs:
        - "I apologize that I usually cannot give personalized responses to students who contact me about joining the lab, particularly those students applying to Ohio State who have not matriculated. You may feel free to contact me, as I will be glad to know that you are interested, but you are likely to receive a form letter in response."
        - "Click below for information on applying to Computer Science & Engineering at Ohio State. Please note the application deadlines, particularly for fellowships."
        - "**On the application process:** complete application files are reviewed by a department-level committee before they are passed to the research areas; I only see files that have made it through the department-level committee. There is a place on the form to indicate that you have contacted me, which will help with routing. However, the strongest case you can make is by making a strong, focused statement of purpose; this is read by multiple faculty members as we make recommendations for admission and funding. The admissions process takes several months. Research areas don't see folders until well into the new year (late January/early February)."
        - "**On funding:** the department uses three sources of funding to help graduate students. Not all students are admitted with funding, unfortunately: we just don't have enough resources for everyone. Strong students are placed in a competition for university-level fellowship funding, which usually covers the first year. The department also makes some offers with Teaching Assistant (TA funding), but these resources vary from year to year. Note that professors (like me) don't have the authority to make fellowship or TA offers. Professors do often have Research Assistantship positions (RA); these positions are often funded by using external research grants. However, I do not usually offer funding to incoming students; see the next section for details."
      name: "Click here for information on applying to Computer Science & Engineering at Ohio State. "
      grad_application_url: "https://cse.osu.edu/graduate/cse-graduate-admissions" 
    - heading: "If you are already admitted to/attending Ohio State"
      paragraphs:
        - "Congratulations! We're glad to have you here."
        - "As mentioned above, I rarely fund first year students in their first term at OSU. This is in part because the external funds available for RAships varies over time and I need to fund student currently in the lab first, and because I like to get to know students and their abilities before putting them into the main line of research. Students also have quite a large coursework committment in their first year; managing that first term can be a challenge for some. It's also important for the student to know that they match my advising style."
        - "**The typical path for grad students to get to know me is to take one of the project-based courses and do well in that:"
      course_list:
        - "CSE 5522: Advanced Artificial Intelligence"
        - "CSE 5525: Foundations of Speech and Language Processing"
        - "CSE 5539: Advanced Studies in Artificial Intelligence"
      final_paragraph: "Students who want to learn more about the area but get closed out of classes might want to enroll for 1 credit of my section of CSE 6539 (typically meets Monday @ 9:10). Students enrolling for credit will be expected to be active participants and contribute to the presentation of papers in the literature."

papers:
  heading: "Selected papers (with commentary)"
  entries:
    - citation: "D. Bagchi, P. Plantinga, A. Stiff, and E. Fosler-Lussier"
      title: "Spectral feature mapping with mimic loss for robust speech recognition,"
      link: "https://arxiv.org/pdf/1803.09816"
      venue: "ICASSP 2018"
      commentary: "For the task of speech enhancement, local learning objectives are agnostic to phonetic structures helpful for speech recognition. We propose to add a global criterion to ensure de-noised speech is useful for downstream tasks like ASR. We first train a spectral classifier on clean speech to predict senone labels. Then, the spectral classifier is joined with our speech enhancer as a noisy speech recognizer. This model is taught to imitate the output of the spectral classifier alone on clean speech. This mimic loss is combined with the traditional local criterion to train the speech enhancer to produce de-noised speech. Feeding the de-noised speech to an off-the-shelf Kaldi training recipe for the CHiME-2 corpus shows significant improvements in WER."
    
    - citation: "D. Newman-Griffis, A. Lai, and E. Fosler-Lussier"
      title: "Jointly embedding entities and text with distant supervision,"
      link: "https://efosler.github.io/"
      venue: "Proceedings of the 3rd Workshop on Representation Learning for NLP, 2018"
      commentary: "Learning representations for knowledge base entities and concepts is becoming increasingly important for NLP applications. However, recent entity embedding methods have relied on structured resources that are expensive to create for new domains and corpora. We present a distantly-supervised method for jointly learning embeddings of entities and text from an unnanotated corpus, using only a list of mappings between entities and surface forms. We learn embeddings from open-domain and biomedical corpora, and compare against prior methods that rely on human-annotated text or large knowledge graph structure. Our embeddings capture entity similarity and relatedness better than prior work, both in existing biomedical datasets and a new Wikipedia-based dataset that we release to the community. Results on analogy completion and entity sense disambiguation indicate that entities and words capture complementary information that can be effectively combined for downstream use."

    - citation: "J.K. Kim, Y.B. Kim, R. Sarikaya, and E. Fosler-Lussier"
      title: "Cross-lingual transfer learning for POS tagging without cross-lingual resources,"
      link: "https://efosler.github.io/"
      venue: "EMNLP 2017"
      commentary: "Training a POS tagging model with crosslingual transfer learning usually requires linguistic knowledge and resources about the relation between the source language and the target language. In this paper, we introduce a cross-lingual transfer learning model for POS tagging without ancillary resources such as parallel corpora. The proposed cross-lingual model utilizes a common BLSTM that enables knowledge transfer from other languages, and private BLSTMs for language-specific representations. The cross-lingual model is trained with language-adversarial training and bidirectional language modeling as auxiliary objectives to better represent language-general information while not losing the information about a specific target language. Evaluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language."

    - citation: "Y. He and E. Fosler-Lussier"
      title:  "Segmental Conditional Random Fields with Deep Neural Networks as Acoustic Models for First-Pass Word Recognition,"
      link: "https://efosler.github.io/papers/scrf_word.pdf"
      venue: "Interspeech 2015, Dresden, Germany, 2015"
      commentary: "In this line of research, my lab engaged in a series of studies to build automatic speech recognition systems using direct discriminative models that can combine correlated evidence of linguistic events. This work is the lastest step in this line of research: it provides a discriminative framework for modeling longer trajectories in speech through segmental models. The innovation in this particular paper is the first one-pass discriminative segmental model for word recognition (building on our previous work in phone recognition). We show that the monophone-based model improves recognition over discriminatively trained monophone-based HMM and Frame-based CRF models for the Wall Street Journal read-speech task, and starts to approach triphone-based performance. Thus, this serves as a good intermediate point in building systems that can start to compete with state-of-the-art systems."

    - citation: "R. Prabhavalkar, E. Fosler-Lussier, and K. Livescu"
      title:  "A Factored Conditional Random Field Model for Articulatory Feature Forced Transcription,"
      link: "https://efosler.github.io/papers/asru2011_prabhavalkar.pdf"
      venue: "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2011"
      commentary: "Recognized as a Spotlight Poster at ASRU 2011 (voted as a top 3 poster in its session by the attendees). Segmental modeling can be thought of as a type of linguistic structural modeling (integrating linguistic structure over time). Another linguistic-inspired modeling approach that we have experimented with, in conjunction with partners at Toyota Technological Institute at Chicago, explicitly models articulator trajectories over time through a factored model -- unlike phone-based systems, this paradigm allows models of asynchrony which can account for different types of pronunciation variation commonly seen in continuous speech. In this paper, we use factorized Conditional Random Fields in order to learn patterns of asynchrony that can be utilized in providing articulatory feature transcriptions that can be expensive to obtain manually. Our experiments show that the transcriptions can better account for pronunciation variations observed by linguists in the Switchboard corpus. In subsequent papers, we were able to utilize this framework for acoustic-based keyword spotting, showing improvement over a HMM-based baseline."

     